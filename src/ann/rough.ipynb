{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f5d55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mk009/DA6401/ae21b036_da6401_assignment-01/src/ann\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c981d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb04e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca7fa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-save_path'], dest='save_path', nargs=None, const=None, default='', type=None, choices=None, required=False, help='--model_save_path: Path to save trained model', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-d', choices=['mnist', 'fashion_mnist'], default= 'mnist', help= '--dataset: \\'mnist\\' or \\'fashion_mnist\\'')\n",
    "parser.add_argument('-e', type= int, default= 1000, help= '--epochs: Number of training epochs')\n",
    "parser.add_argument('-b', type= int, default= 64, help= '--batch_size: Mini-batch size')\n",
    "parser.add_argument('-lr', type= float, default= 1e-3, help= '--learning_rate: Learning rate for optimizer')\n",
    "parser.add_argument('-o', choices=['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam'], default='sgd', help= '--optimizer: \\'sgd\\', \\'momentum\\', \\'nag\\', \\'rmsprop\\', \\'adam\\', \\'nadam\\'' )\n",
    "parser.add_argument('-nhl', type=int, default=2, help= '--num_layers: Number of hidden layers')\n",
    "parser.add_argument('-sz', type=int, nargs='+', help= '--hidden_size: Number of neurons in each hidden layer (list of values)')\n",
    "parser.add_argument('-a', choices= ['sigmoid', 'tanh', 'relu'], default='sigmoid', help= '--activation: choice of sigmoid, tanh, relu')\n",
    "parser.add_argument('-l', choices=['mean_squared_error', 'cross_entropy'], default='mean_squared_error', help= 'Loss function (\\'cross_entropy\\', \\'mse\\')')\n",
    "parser.add_argument('-w_i', choices= ['random', 'xavier'], default='random', help= '--weight_init: choice of random or xavier')\n",
    "parser.add_argument('-wd', type= float, default=0., help= '--weight_decay: Weight decay for L2 regularization')\n",
    "parser.add_argument('-wb_project', default= '', help= '--wandb_project: W&B project name')\n",
    "parser.add_argument('-save_path', default='', help= '--model_save_path: Path to save trained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb78a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e5c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.nhl = 2\n",
    "args.sz = [512,512]\n",
    "args.l = 'cross_entropy'\n",
    "args.wd = 0.001\n",
    "args.a = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17433a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-23 18:26:48.480106: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-23 18:26:48.519946: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-23 18:26:49.626535: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b4ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62819970",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(28*28, 10, Softmax, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beeeec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/100]\n",
      "  Batch [211/211] Loss: 0.856767\n",
      "Epoch [1/100] Train Loss: 1.933117 | Val Loss: 0.844195 | Val Acc: 0.7488\n",
      "\n",
      "Epoch [2/100]\n",
      "  Batch [211/211] Loss: 0.403944\n",
      "Epoch [2/100] Train Loss: 0.565074 | Val Loss: 0.431198 | Val Acc: 0.8802\n",
      "\n",
      "Epoch [3/100]\n",
      "  Batch [211/211] Loss: 0.283501\n",
      "Epoch [3/100] Train Loss: 0.382807 | Val Loss: 0.354294 | Val Acc: 0.8985\n",
      "\n",
      "Epoch [4/100]\n",
      "  Batch [211/211] Loss: 0.390203\n",
      "Epoch [4/100] Train Loss: 0.325683 | Val Loss: 0.318059 | Val Acc: 0.9122\n",
      "\n",
      "Epoch [5/100]\n",
      "  Batch [211/211] Loss: 0.289086\n",
      "Epoch [5/100] Train Loss: 0.287800 | Val Loss: 0.285691 | Val Acc: 0.9192\n",
      "\n",
      "Epoch [6/100]\n",
      "  Batch [211/211] Loss: 0.229086\n",
      "Epoch [6/100] Train Loss: 0.251798 | Val Loss: 0.252715 | Val Acc: 0.9275\n",
      "\n",
      "Epoch [7/100]\n",
      "  Batch [211/211] Loss: 0.198144\n",
      "Epoch [7/100] Train Loss: 0.221233 | Val Loss: 0.239240 | Val Acc: 0.9313\n",
      "\n",
      "Epoch [8/100]\n",
      "  Batch [211/211] Loss: 0.134899\n",
      "Epoch [8/100] Train Loss: 0.195080 | Val Loss: 0.201549 | Val Acc: 0.9427\n",
      "\n",
      "Epoch [9/100]\n",
      "  Batch [211/211] Loss: 0.227623\n",
      "Epoch [9/100] Train Loss: 0.175460 | Val Loss: 0.185607 | Val Acc: 0.9478\n",
      "\n",
      "Epoch [10/100]\n",
      "  Batch [211/211] Loss: 0.121688\n",
      "Epoch [10/100] Train Loss: 0.159195 | Val Loss: 0.177392 | Val Acc: 0.9490\n",
      "\n",
      "Epoch [11/100]\n",
      "  Batch [211/211] Loss: 0.119623\n",
      "Epoch [11/100] Train Loss: 0.145870 | Val Loss: 0.160189 | Val Acc: 0.9530\n",
      "\n",
      "Epoch [12/100]\n",
      "  Batch [211/211] Loss: 0.162904\n",
      "Epoch [12/100] Train Loss: 0.133988 | Val Loss: 0.155387 | Val Acc: 0.9557\n",
      "\n",
      "Epoch [13/100]\n",
      "  Batch [211/211] Loss: 0.118214\n",
      "Epoch [13/100] Train Loss: 0.124730 | Val Loss: 0.142222 | Val Acc: 0.9600\n",
      "\n",
      "Epoch [14/100]\n",
      "  Batch [211/211] Loss: 0.143489\n",
      "Epoch [14/100] Train Loss: 0.116030 | Val Loss: 0.137536 | Val Acc: 0.9605\n",
      "\n",
      "Epoch [15/100]\n",
      "  Batch [211/211] Loss: 0.054560\n",
      "Epoch [15/100] Train Loss: 0.109168 | Val Loss: 0.129809 | Val Acc: 0.9632\n",
      "\n",
      "Epoch [16/100]\n",
      "  Batch [211/211] Loss: 0.122962\n",
      "Epoch [16/100] Train Loss: 0.102317 | Val Loss: 0.123963 | Val Acc: 0.9630\n",
      "\n",
      "Epoch [17/100]\n",
      "  Batch [211/211] Loss: 0.082213\n",
      "Epoch [17/100] Train Loss: 0.096286 | Val Loss: 0.120173 | Val Acc: 0.9658\n",
      "\n",
      "Epoch [18/100]\n",
      "  Batch [211/211] Loss: 0.070998\n",
      "Epoch [18/100] Train Loss: 0.091390 | Val Loss: 0.116325 | Val Acc: 0.9652\n",
      "\n",
      "Epoch [19/100]\n",
      "  Batch [211/211] Loss: 0.074655\n",
      "Epoch [19/100] Train Loss: 0.086723 | Val Loss: 0.112194 | Val Acc: 0.9657\n",
      "\n",
      "Epoch [20/100]\n",
      "  Batch [211/211] Loss: 0.128982\n",
      "Epoch [20/100] Train Loss: 0.082607 | Val Loss: 0.110587 | Val Acc: 0.9660\n",
      "\n",
      "Epoch [21/100]\n",
      "  Batch [211/211] Loss: 0.088932\n",
      "Epoch [21/100] Train Loss: 0.079197 | Val Loss: 0.116257 | Val Acc: 0.9662\n",
      "\n",
      "Epoch [22/100]\n",
      "  Batch [211/211] Loss: 0.052817\n",
      "Epoch [22/100] Train Loss: 0.075777 | Val Loss: 0.101343 | Val Acc: 0.9695\n",
      "\n",
      "Epoch [23/100]\n",
      "  Batch [211/211] Loss: 0.081666\n",
      "Epoch [23/100] Train Loss: 0.072840 | Val Loss: 0.099762 | Val Acc: 0.9697\n",
      "\n",
      "Epoch [24/100]\n",
      "  Batch [211/211] Loss: 0.061639\n",
      "Epoch [24/100] Train Loss: 0.069783 | Val Loss: 0.097518 | Val Acc: 0.9702\n",
      "\n",
      "Epoch [25/100]\n",
      "  Batch [211/211] Loss: 0.040883\n",
      "Epoch [25/100] Train Loss: 0.066987 | Val Loss: 0.098981 | Val Acc: 0.9703\n",
      "\n",
      "Epoch [26/100]\n",
      "  Batch [211/211] Loss: 0.045542\n",
      "Epoch [26/100] Train Loss: 0.064508 | Val Loss: 0.095113 | Val Acc: 0.9705\n",
      "\n",
      "Epoch [27/100]\n",
      "  Batch [211/211] Loss: 0.057834\n",
      "Epoch [27/100] Train Loss: 0.062243 | Val Loss: 0.095975 | Val Acc: 0.9703\n",
      "\n",
      "Epoch [28/100]\n",
      "  Batch [211/211] Loss: 0.103157\n",
      "Epoch [28/100] Train Loss: 0.060576 | Val Loss: 0.091854 | Val Acc: 0.9727\n",
      "\n",
      "Epoch [29/100]\n",
      "  Batch [211/211] Loss: 0.038854\n",
      "Epoch [29/100] Train Loss: 0.058777 | Val Loss: 0.089703 | Val Acc: 0.9737\n",
      "\n",
      "Epoch [30/100]\n",
      "  Batch [211/211] Loss: 0.046588\n",
      "Epoch [30/100] Train Loss: 0.056642 | Val Loss: 0.088126 | Val Acc: 0.9738\n",
      "\n",
      "Epoch [31/100]\n",
      "  Batch [211/211] Loss: 0.040385\n",
      "Epoch [31/100] Train Loss: 0.055167 | Val Loss: 0.087433 | Val Acc: 0.9727\n",
      "\n",
      "Epoch [32/100]\n",
      "  Batch [28/211] Loss: 0.049372\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m NN.optimizer.lr = \u001b[32m0.1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mNN\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DA6401/ae21b036_da6401_assignment-01/src/ann/neural_network.py:116\u001b[39m, in \u001b[36mNeuralNetwork.train\u001b[39m\u001b[34m(self, X_train, y_train, epochs, batch_size, shuffle)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     y_hat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.backward(y, y_hat)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mself\u001b[39m.update_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DA6401/ae21b036_da6401_assignment-01/src/ann/neural_network.py:64\u001b[39m, in \u001b[36mNeuralNetwork.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     60\u001b[39m hk_1 = X\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_layers+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhk_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     hk_1 = \u001b[38;5;28mself\u001b[39m.Layers[k].hk\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m.Layers[-\u001b[32m1\u001b[39m].hk = \u001b[38;5;28mself\u001b[39m.output_act.forward(\u001b[38;5;28mself\u001b[39m.Layers[-\u001b[32m1\u001b[39m].hk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DA6401/ae21b036_da6401_assignment-01/src/ann/neural_layer.py:39\u001b[39m, in \u001b[36mneural_layer.forward\u001b[39m\u001b[34m(self, hk_1)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hk_1):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mself\u001b[39m.hk_1 = hk_1\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28mself\u001b[39m.ak = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhk_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mself\u001b[39m.ak+= \u001b[38;5;28mself\u001b[39m.b\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m.hk = \u001b[38;5;28mself\u001b[39m.activation.forward(\u001b[38;5;28mself\u001b[39m.ak)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "NN.optimizer.lr = 0.1\n",
    "NN.train(x_train,y_train,100,256,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49cb05bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation\n",
      "\n",
      "Eval Loss: 0.066549 | Accuracy: 0.9799\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = Dataloader(x_test,y_test,64,False,True,True)\n",
    "total_loss = 0.0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "print(\"\\nEvaluation\")\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for i, (X, y) in enumerate(eval_dataloader):\n",
    "\n",
    "    y_hat = NN.forward(X)\n",
    "\n",
    "    loss = NN.objective.loss(y, y_hat)\n",
    "\n",
    "    total_loss += np.sum(loss)\n",
    "\n",
    "    preds = np.argmax(y_hat, axis=1)\n",
    "    true_labels = np.argmax(y, axis=1)\n",
    "\n",
    "    total_correct += np.sum(preds == true_labels)\n",
    "    total_samples += y.shape[0]\n",
    "\n",
    "avg_loss = total_loss / total_samples\n",
    "accuracy = total_correct / total_samples\n",
    "\n",
    "print(f\"\\nEval Loss: {avg_loss:.6f} | Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98689acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48480a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 grad max: 194545\n",
      "Layer 1 grad max: 0.00021254309035347027\n",
      "Layer 0 input max: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 0 grad max:\", np.argmax((NN.Layers[0].grad_W)),axis)\n",
    "print(\"Layer 1 grad max:\", np.max(np.abs(NN.Layers[1].grad_W)))\n",
    "\n",
    "print(\"Layer 0 input max:\", np.max(np.abs(NN.Layers[0].hk_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bd1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da6401",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
